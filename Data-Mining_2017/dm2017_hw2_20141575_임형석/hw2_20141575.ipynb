{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MINST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ./MINST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./MINST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MINST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "learning_rate=0.001\n",
    "training_epochs=15\n",
    "batch_size=128\n",
    "SUMMARY_DIR='./mnist'\n",
    "\n",
    "#load mnist dataset\n",
    "MNIST=input_data.read_data_sets(\"./MINST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0부터 examples수 만큼의 list\n",
    "example_list = np.arange(0,MNIST.train.num_examples)\n",
    "\n",
    "def get_train_batch(batch_size) :\n",
    "    start = i * batch_size   #시작 index 저장 \n",
    "    if i == 0 :\n",
    "        np.random.shuffle(example_list)   #idx list를 shuffle        \n",
    "    if i != batch_cnt :\n",
    "            end = start + batch_size\n",
    "    else :                  #마지막 mini batch의 end index 설정\n",
    "            end = start + int(MNIST.train.num_examples)%batch_size\n",
    "    local_id = example_list[start:end]\n",
    "    x = [MNIST.train._images[j] for j in local_id]\n",
    "    y = [MNIST.train._labels[k] for k in local_id]\n",
    "        \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('input') as scope:\n",
    "    X=tf.placeholder(tf.float32, [None, 784], name='image')\n",
    "    y=tf.placeholder(tf.float32, [None, 10], name='label')\n",
    "\n",
    "with tf.variable_scope('layer1') as scope:\n",
    "    W1=tf.get_variable(\"W\", shape=[784,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1=tf.Variable(tf.random_normal([512]))\n",
    "    L1=tf.nn.relu(tf.add(tf.matmul(X, W1), b1))\n",
    "    \n",
    "    tf.summary.histogram(\"X\", X)\n",
    "    tf.summary.histogram(\"weights\", W1)\n",
    "    tf.summary.histogram(\"bias\", b1)\n",
    "    tf.summary.histogram(\"layer\", L1)\n",
    "\n",
    "with tf.variable_scope('layer2') as scope:\n",
    "    W2=tf.get_variable(\"W\", shape=[512,1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2=tf.Variable(tf.random_normal([1024]))\n",
    "    L2=tf.nn.relu(tf.add(tf.matmul(L1, W2), b2))\n",
    "    \n",
    "    tf.summary.histogram(\"X\", X)\n",
    "    tf.summary.histogram(\"weights\", W2)\n",
    "    tf.summary.histogram(\"bias\", b2)\n",
    "    tf.summary.histogram(\"layer\", L2)\n",
    "    \n",
    "with tf.variable_scope('layer3') as scope:\n",
    "    W3=tf.get_variable(\"W\", shape=[1024,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3=tf.Variable(tf.random_normal([10]))\n",
    "    y_=tf.add(tf.matmul(L2, W3), b3)\n",
    "    \n",
    "    tf.summary.histogram(\"weights\", W3)\n",
    "    tf.summary.histogram(\"bias\", b3)\n",
    "    tf.summary.histogram(\"logits\", y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "#merge all summaries\n",
    "summary=tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 loss= 2.098340\n",
      "Epoch: 02 loss= 1.714840\n",
      "Epoch: 03 loss= 1.407824\n",
      "Epoch: 04 loss= 1.164516\n",
      "Epoch: 05 loss= 0.984222\n",
      "Epoch: 06 loss= 0.855008\n",
      "Epoch: 07 loss= 0.762069\n",
      "Epoch: 08 loss= 0.693061\n",
      "Epoch: 09 loss= 0.640902\n",
      "Epoch: 10 loss= 0.600035\n",
      "Epoch: 11 loss= 0.567196\n",
      "Epoch: 12 loss= 0.540224\n",
      "Epoch: 13 loss= 0.517976\n",
      "Epoch: 14 loss= 0.498961\n",
      "Epoch: 15 loss= 0.482675\n",
      "Test accuracy: 0.884\n"
     ]
    }
   ],
   "source": [
    "global_step=0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer=tf.summary.FileWriter(SUMMARY_DIR, sess.graph)\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch=int(MNIST.train.num_examples/batch_size)+1\n",
    "        batch_cnt = total_batch - 1\n",
    "        avg_loss=0\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = get_train_batch(batch_size)   \n",
    "        # batch_xs, batch_ys = MNIST.train.next_batch(batch_size)\n",
    "            feed_dict={X:batch_xs, y:batch_ys}\n",
    "            s, l, _=sess.run([summary, loss, optimizer], feed_dict=feed_dict)\n",
    "            writer.add_summary(s, global_step=global_step)\n",
    "            global_step+=1\n",
    "            avg_loss+=l\n",
    "        print('Epoch:','%02d'%(epoch+1),'loss=','{:.6f}'.format(avg_loss/total_batch))\n",
    "    \n",
    "    correct_prediction=tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    acc=sess.run(accuracy, feed_dict={X:MNIST.test.images, y:MNIST.test.labels})\n",
    "    print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
